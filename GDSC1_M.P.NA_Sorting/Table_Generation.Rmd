---
title: "Table Generation"
author: "Alex"
date: "5/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(tidyr)
require(reshape2)
require(readxl)
require(dplyr)
library(ggplot2)
require(ggplot2)
require(data.table)
require(dplyr)
```

Data Acquisition
```{r echo = T, results = 'hide', error=FALSE, warning=FALSE, message=FALSE}
inputdata <- read_excel("./inputdata.xlsx") #, sheet = "raw_data")

fittedinputdata <- read_excel("inputdata.xlsx", sheet = "fitted_data")

#View(inputdata)
#View(fittedinputdata)
```

```{r}
RawData <- inputdata[2]
is.data.frame(RawData)

RawData <- cbind(RawData, inputdata["COSMIC_ID"], inputdata["CELL_LINE_NAME"], inputdata["DRUG_ID"], inputdata["CELL_ID"], inputdata["MASTER_CELL_ID"], inputdata["SEEDING_DENSITY"], inputdata["ASSAY"], inputdata["DURATION"], inputdata["POSITION"], inputdata["TAG"], inputdata["CONC"], inputdata["INTENSITY"])

#Cleaning Up
CleanData <- RawData

CleanData[CleanData == "NA"] <- "0"
#CleanData <- subset(CleanData, CleanData$TAG != "UN-USED")
```
Parsing and Subsetting  
```{r echo = T, results = 'hide', error=FALSE, warning=FALSE, message=FALSE, results=FALSE}
ChunkStarts <- which(grepl("B", CleanData$TAG))
print(ChunkStarts)

#ChunkStarts <- c(1, 13)

for (i in ChunkStarts){
  Tempterm <- i+10
  ProcessingSubset <- CleanData %>% slice(i:Tempterm)
  
  #Normalization Calculations
  Tempdata <- ProcessingSubset["CONC"]
  Tempdata <- cbind(Tempdata, ProcessingSubset["INTENSITY"])
  Tempdata[Tempdata == "NA"] <- "0"
  Tempdata <- as.data.frame(Tempdata)
  Tempvector <- as.vector(Tempdata$INTENSITY)
  Tempterm <- as.numeric(print(paste(Tempdata[1,2])))
  Tempterm2 <- as.numeric(print(paste(Tempdata[11,2])))
  
  ProcessingSubset$CONC <- as.numeric(ProcessingSubset$CONC)
  ProcessingSubset["lnCONC"] <- log(ProcessingSubset["CONC"])
  ProcessingSubset["RELATIVE_VIABILITY"] <- (Tempvector - Tempterm)/(Tempterm2 - Tempterm)
 
#  ProcessingSubset$CONC <- as.vector(ProcessingSubset$CONC)
#  ProcessingSubset[ProcessingSubset$lnCONC == "-Inf"] <- "0"
  #Naming and separating chunks into individual Cell/Drug data.frames
  TempName <- paste(ProcessingSubset$CELL_LINE_NAME[1], ProcessingSubset$DRUG_ID[2], sep = "_")
  TempName <- as.vector(TempName)
  assign(TempName[1], ProcessingSubset)
}
```

```{r}
library(deSolve)
library(glue)
library(CARRoT)
library(caret)
library(nlme)

#Simple equation: I = [D]/([D] + IC 50 )  --> (IC50 = [D]-[D]I)/I)
#equation.test.data <- slice(`MC-CAR_104`[2:11,])
#equation.test.data <- chop(equation.test.data[,11:15])

D <- as.vector(`MC-CAR_104`$CONC[2:11])
I <- as.vector(`MC-CAR_104`$RELATIVE_VIABILITY[2:11])
Tempdata <- as.data.frame(D)
Tempdata <- cbind(Tempdata, I)

Tempvector <- D/I-D
#equation.test.data$TempLC50 <- Tempvector
Tempvector <- as.vector(Tempvector)
# Build the model
model <- gnls(I ~ D/(D+Tempvector), start=c(D=D, Tempvector=Tempvector))
# Make predictions
predictions.gnls <- model %>% prediction.gnls(equation.test.data)
# Model performance
#data.frame(
  #RMSE = RMSE(predictions, Tempdata$I),
  #R2 = R2(predictions, Tempdata$I))



```

# medv = y, lstat = x
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %/% createDataPartition(Boston$medv, p = 1, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

nls(medv ~ lstat, data = test.data, start=list(lstat=0))

# Build the model
model <- nls(medv ~ lstat^2, data = test.data, start=list(lstat=0))
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv))

ggplot(test.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = nls, formula = y ~ x^2)

```
Poly Modeling:
#ggplot(train.data, aes(lstat, medv) ) +
#  geom_point() +
#  stat_smooth()



ggplot(test.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ poly(x, 5, raw = TRUE))


Linear Modeling:
# Build the model
model <- lm(medv ~ lstat, data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv))

ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ x)

```  
# R Documentation Example
initLogis <- function(mCall, data, LHS) {
    xy <- data.frame(sortedXyData(mCall[["input"]], LHS, data))
    if(nrow(xy) < 4)
        stop("too few distinct input values to fit a logistic model")
    z <- xy[["y"]]
    ## transform to proportion, i.e. in (0,1) :
    rng <- range(z); dz <- diff(rng)
    z <- (z - rng[1L] + 0.05 * dz)/(1.1 * dz)
    xy[["z"]] <- log(z/(1 - z))		# logit transformation
    aux <- coef(lm(x ~ z, xy))
    pars <- coef(nls(y ~ 1/(1 + exp((xmid - x)/scal)),
                     data = xy,
                     start = list(xmid = aux[[1L]], scal = aux[[2L]]),
                     algorithm = "plinear"))
    setNames(pars[c(".lin", "xmid", "scal")], nm = mCall[c("Asym", "xmid", "scal")])
}

SSlogis <- selfStart(~ Asym/(1 + exp((xmid - x)/scal)),
                     initial = initLogis,
                     parameters = c("Asym", "xmid", "scal"))




summary(SSlogis(D, xmid, scal = 0))


```


